{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and getting a sense of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's load the necessary libraries for importing the data and processing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the dataset and take a look at some of the different articles and their preclassified category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>politics</td>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>politics</td>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sport</td>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sport</td>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text\n",
       "0           tech  tv future in the hands of viewers with home th...\n",
       "1       business  worldcom boss  left books alone  former worldc...\n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
       "3          sport  yeading face newcastle in fa cup premiership s...\n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...\n",
       "5       politics  howard hits back at mongrel jibe michael howar...\n",
       "6       politics  blair prepares to name poll date tony blair is...\n",
       "7          sport  henman hopes ended in dubai third seed tim hen...\n",
       "8          sport  wilkinson fit to face edinburgh england captai...\n",
       "9  entertainment  last star wars  not for children  the sixth an..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('bbc-text.csv')\n",
    "articles.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it would be a good idea to check the number of articles in each category. This way we can look for class imbalance that may affect our model. In case you cannot or do not want to install cufflings and plotly a static picture is also attached below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": true
       },
       "data": [
        {
         "marker": {
          "color": "rgba(255, 153, 51, 0.6)",
          "line": {
           "color": "rgba(255, 153, 51, 1.0)",
           "width": 1
          }
         },
         "name": "category",
         "orientation": "v",
         "text": "",
         "type": "bar",
         "x": [
          "sport",
          "business",
          "politics",
          "tech",
          "entertainment"
         ],
         "y": [
          511,
          510,
          417,
          401,
          386
         ]
        }
       ],
       "layout": {
        "legend": {
         "bgcolor": "#F5F6F9",
         "font": {
          "color": "#4D5663"
         }
        },
        "paper_bgcolor": "#F5F6F9",
        "plot_bgcolor": "#F5F6F9",
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "color": "#4D5663"
         },
         "text": "Number of articles in each category"
        },
        "xaxis": {
         "gridcolor": "#E1E5ED",
         "showgrid": true,
         "tickfont": {
          "color": "#4D5663"
         },
         "title": {
          "font": {
           "color": "#4D5663"
          },
          "text": ""
         },
         "zerolinecolor": "#E1E5ED"
        },
        "yaxis": {
         "gridcolor": "#E1E5ED",
         "showgrid": true,
         "tickfont": {
          "color": "#4D5663"
         },
         "title": {
          "font": {
           "color": "#4D5663"
          },
          "text": "Number of Articles"
         },
         "zerolinecolor": "#E1E5ED"
        }
       }
      },
      "text/html": [
       "<div>\n",
       "        \n",
       "        \n",
       "            <div id=\"96af0b06-c28a-424c-83bd-2630e5268b47\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
       "            <script type=\"text/javascript\">\n",
       "                require([\"plotly\"], function(Plotly) {\n",
       "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
       "                    window.PLOTLYENV.BASE_URL='https://plot.ly';\n",
       "                    \n",
       "                if (document.getElementById(\"96af0b06-c28a-424c-83bd-2630e5268b47\")) {\n",
       "                    Plotly.newPlot(\n",
       "                        '96af0b06-c28a-424c-83bd-2630e5268b47',\n",
       "                        [{\"marker\": {\"color\": \"rgba(255, 153, 51, 0.6)\", \"line\": {\"color\": \"rgba(255, 153, 51, 1.0)\", \"width\": 1}}, \"name\": \"category\", \"orientation\": \"v\", \"text\": \"\", \"type\": \"bar\", \"x\": [\"sport\", \"business\", \"politics\", \"tech\", \"entertainment\"], \"y\": [511, 510, 417, 401, 386]}],\n",
       "                        {\"legend\": {\"bgcolor\": \"#F5F6F9\", \"font\": {\"color\": \"#4D5663\"}}, \"paper_bgcolor\": \"#F5F6F9\", \"plot_bgcolor\": \"#F5F6F9\", \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"Number of articles in each category\"}, \"xaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"\"}, \"zerolinecolor\": \"#E1E5ED\"}, \"yaxis\": {\"gridcolor\": \"#E1E5ED\", \"showgrid\": true, \"tickfont\": {\"color\": \"#4D5663\"}, \"title\": {\"font\": {\"color\": \"#4D5663\"}, \"text\": \"Number of Articles\"}, \"zerolinecolor\": \"#E1E5ED\"}},\n",
       "                        {\"showLink\": true, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\", \"responsive\": true}\n",
       "                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('96af0b06-c28a-424c-83bd-2630e5268b47');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })\n",
       "                };\n",
       "                });\n",
       "            </script>\n",
       "        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Run the code below if you want to have the interactive plots. \n",
    "Keep in mind that you have to install the libraries at your command line/terminal using:\n",
    "\n",
    "    pip install plotly\n",
    "    pip install cufflinks\n",
    "\n",
    "** NOTE: Make sure you only have one installation of Python on your computer when you do this, otherwise the \n",
    "installation may not work. **'''\n",
    "\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import cufflinks as cf\n",
    "\n",
    "print(__version__) # requires version >= 1.9.0\n",
    "init_notebook_mode(connected=True)\n",
    "cf.go_offline()\n",
    "\n",
    "articles['category'].value_counts().sort_values(ascending=False).iplot(kind='bar', yTitle='Number of Articles', \n",
    "                                                                title='Number of articles in each category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Screenshots/no_of_categories.png\" width='940' height='300'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some imbalance between the classes but it is not significant.\n",
    "\n",
    "### Text preprocessing\n",
    "\n",
    "As to all projects in data science, a critical part of analysis is preprocessing. In text analysis, our goal is to create a corpus which will be consisted of different words that will make up our vocabulary. We need to make that vocabulary as small as possible but at the same time, keep all of the essential information and \"key\" words for labelling. As a result, we will remove a category of words called \"stopwords\", which are words commonly used in our vocabulary but do not add any value to our model when it comes to classifying the text to a specific category. Let's see which these words are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'for', 'she', 'there', 'nor', 'mightn', 'whom', 'wouldn', 'themselves', 'had', 'those', 'them', \"haven't\", \"couldn't\", 'wasn', 'ma', 'down', \"needn't\", 'further', 'be', 'above', 'couldn', 'he', 'doing', 'ours', 'up', 'you', 'shouldn', 'or', 'just', 'against', 'can', 'what', \"won't\", 'off', 'when', 'so', 'who', 'each', 'ain', 'in', 'myself', 'am', \"it's\", 'having', 'any', 'between', 'yourselves', 'a', \"wasn't\", 'other', \"you'd\", 'on', 'too', \"don't\", 'out', 'herself', 'our', 'were', 'as', 'about', \"that'll\", 'few', 'such', 'hasn', 'their', \"she's\", 'that', 'me', 'these', 'some', 'been', 'then', 't', \"shouldn't\", 'will', 'o', 'over', 'have', 'at', 'which', 'once', 'hadn', 'no', 'now', 'only', 'and', 'during', 'its', 'here', 'of', 'more', 'before', 'weren', 'until', 've', 'by', 'very', 'because', 'yourself', \"weren't\", 'mustn', 'same', 'd', 'into', 'where', 'it', 'most', 'all', \"mustn't\", 'himself', 'doesn', 'under', 'to', 'below', 'if', \"hadn't\", 'theirs', 'has', \"you're\", \"shan't\", 'but', 'again', 'why', \"mightn't\", 'shan', 'is', 'ourselves', \"aren't\", 'was', 'being', 'him', 'hers', 'did', 'this', 'aren', 'your', 'didn', 'my', 'her', 'isn', \"didn't\", 'not', 'should', 'own', 'the', 'an', 'do', 'after', 'haven', 'needn', 'don', 'll', 'i', 's', \"wouldn't\", 'does', 'm', 'yours', \"you've\", 'won', 'we', 're', 'through', 'how', 'both', 'itself', \"should've\", 'are', \"doesn't\", \"you'll\", 'y', \"isn't\", 'they', 'from', \"hasn't\", 'his', 'while', 'with', 'than'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to seperate each article's text from its corresponding label. For our analysis, the text is the \"independent variable\" and the label is the target or \"dependent variable\". Thus, we will train our model to read a text and predict whether it falls under the sport, business, politics, tech, or entertainment category.\n",
    "\n",
    "During this seperation we will also proceed with cleaning the dataset by removing the stopwords we mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "categories = []\n",
    "\n",
    "with open(\"bbc-text.csv\", 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        categories.append(row[0])\n",
    "        article = row[1]\n",
    "        for word in STOPWORDS:\n",
    "            token = ' ' + word + ' '\n",
    "            article = article.replace(token, ' ')\n",
    "            article = article.replace('  ', ' ')\n",
    "        text.append(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a good grasp of what exaclty we have achieved, let's print one of the articles before and after the removal of stopwords. That way we can see the actual value this removal brings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: entertainment\n",
      "\n",
      "\n",
      "ocean s twelve raids box office ocean s twelve  the crime caper sequel starring george clooney  brad pitt and julia roberts  has gone straight to number one in the us box office chart.  it took $40.8m (£21m) in weekend ticket sales  according to studio estimates. the sequel follows the master criminals as they try to pull off three major heists across europe. it knocked last week s number one  national treasure  into third place. wesley snipes  blade: trinity was in second  taking $16.1m (£8.4m). rounding out the top five was animated fable the polar express  starring tom hanks  and festive comedy christmas with the kranks.  ocean s twelve box office triumph marks the fourth-biggest opening for a december release in the us  after the three films in the lord of the rings trilogy. the sequel narrowly beat its 2001 predecessor  ocean s eleven which took $38.1m (£19.8m) on its opening weekend and $184m (£95.8m) in total. a remake of the 1960s film  starring frank sinatra and the rat pack  ocean s eleven was directed by oscar-winning director steven soderbergh. soderbergh returns to direct the hit sequel which reunites clooney  pitt and roberts with matt damon  andy garcia and elliott gould. catherine zeta-jones joins the all-star cast.  it s just a fun  good holiday movie   said dan fellman  president of distribution at warner bros. however  us critics were less complimentary about the $110m (£57.2m) project  with the los angeles times labelling it a  dispiriting vanity project . a milder review in the new york times dubbed the sequel  unabashedly trivial .\n",
      "Length:  1579\n",
      "===================================================================================================================\n",
      "ocean twelve raids box office ocean twelve crime caper sequel starring george clooney brad pitt julia roberts gone straight number one us box office chart. took $40.8m (Â£21m) weekend ticket sales according studio estimates. sequel follows master criminals try pull three major heists across europe. knocked last week number one national treasure third place. wesley snipes blade: trinity second taking $16.1m (Â£8.4m). rounding top five animated fable polar express starring tom hanks festive comedy christmas kranks. ocean twelve box office triumph marks fourth-biggest opening december release us three films lord rings trilogy. sequel narrowly beat 2001 predecessor ocean eleven took $38.1m (Â£19.8m) opening weekend $184m (Â£95.8m) total. remake 1960s film starring frank sinatra rat pack ocean eleven directed oscar-winning director steven soderbergh. soderbergh returns direct hit sequel reunites clooney pitt roberts matt damon andy garcia elliott gould. catherine zeta-jones joins all-star cast. fun good holiday movie said dan fellman president distribution warner bros. however us critics less complimentary $110m (Â£57.2m) project los angeles times labelling dispiriting vanity project . milder review new york times dubbed sequel unabashedly trivial .\n",
      "Length:  1264\n"
     ]
    }
   ],
   "source": [
    "def print_plot(index):\n",
    "    example = articles[articles.index == index][['text', 'category']].values[0]\n",
    "    if len(example) > 0:\n",
    "        print('Product:', example[1])\n",
    "        print('\\n')\n",
    "        print(example[0])\n",
    "        \n",
    "def comparison(i):\n",
    "    print_plot(i)\n",
    "    print('Length: ',len(articles.iloc[i][1]))\n",
    "    print(115*'=')\n",
    "    print(text[i]) \n",
    "    print('Length: ',len(text[i]))\n",
    "\n",
    "comparison(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading both texts we can easily understand what each one is trying to describe, but on the second text we have kept only the \"key\" words. Mind the difference at the length of the article. Now we have a more condense but equally informative form. It is obvious though that there are a lot of things that we can ommit like punctuation and special characters. We will take care of this shortly.\n",
    "\n",
    "At this point, it would be a good idea to set the hyperparameters we will use throughout our code to make it easier to change and edit in the future if needed, since each one of them is totally tunable. We will explain how each hyperparameter works when we get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['business', 'entertainment', 'politics', 'sport', 'tech']\n"
     ]
    }
   ],
   "source": [
    "# --Hyperparameters--\n",
    "\n",
    "vocab_size = 5000\n",
    "embedding_dim = 100\n",
    "trunc_type = 'post'\n",
    "padding_type = 'post'\n",
    "oov_tok = '<UW>'\n",
    "labels=sorted(set(categories))\n",
    "print('Labels: ',labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to encode our labels. Right now our target values are words. Since we are feeding this data to a deep learning model, each category needs to be an array where all of the categories -except the one corresponding to the specific article- will be zero. This can be easily done through keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Encoding the labels -- \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder_categories = LabelEncoder()\n",
    "categories = labelencoder_categories.fit_transform(categories)\n",
    "categories = keras.utils.to_categorical(categories, num_classes=5, dtype='int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand exactly the transformation from words to arrays of zeros and ones we can see the picture below:\n",
    "\n",
    "<img src=\"Screenshots/label_encoding.png\" width='600' height='300'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to split our dataset into a training set and a test set. There are 2,225 news articles in the data, we split them into training set and test set, 80% for training, 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles in training set: 1780\n",
      "Number of labels in training set: 1780\n",
      "Number of articles in test set: 445\n",
      "Number of labels in test set: 445\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_text, test_text, train_labels, test_labels = train_test_split(text, categories, train_size = 0.8, random_state = 0)\n",
    "\n",
    "print(\"Number of articles in training set: \" + str(len(train_text)))\n",
    "print(\"Number of labels in training set: \" + str(len(train_labels)))\n",
    "print(\"Number of articles in test set: \" + str(len(test_text)))\n",
    "print(\"Number of labels in test set: \" + str(len(test_labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step, is one of the most essential steps in the process and it involves the [tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) of our text. For this purpose, we will use the Tokenizer class from keras. This class allows to vectorize a text corpus, by turning each text into either a sequence of integers (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, based on different methods. It is also cleaning our text further more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Unique Tokens:  27233\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                      lower=True)\n",
    "tokenizer.fit_on_texts(train_text)\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of Unique Tokens: ',len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain a little bit further all of the arguments we used for the Tokenizer:\n",
    "\n",
    "* num_words: the maximum number of words to keep, based on word frequency. This is basically our vocabulary which will be equal to 5000 words.\n",
    "* filters: a string where each element is a character that will be\n",
    "    filtered from the texts. We used the *default*, which is all punctuation, plus\n",
    "    tabs and line breaks, minus the `'` character.\n",
    "* lower: boolean. Whether to convert the texts to lowercase. We set this to True.\n",
    "* oov_token: puts a special value in when an unseen word is encountered. This means we want `<UW>` ('Unknown Word') to be used for words that are not in our vocabulary and hence not in the `word_index. fit_on_text`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps are part of preparing our data for our neural networks. Firstly, we have to turn those tokens into lists of sequence. Secondly, we need to bring those sequences to the same length. To achieve that, we will use a technique called \"padding\", a practice which includes adding data to the beginning or end of our texts in order to bring them to the same size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of articles: 230\n"
     ]
    }
   ],
   "source": [
    "# Turning tokens into lists of sequence\n",
    "train_text_sequences = tokenizer.texts_to_sequences(train_text)\n",
    "\n",
    "# Setting as max_length the mean length of the cleaned articles\n",
    "max_length = int(sum([len(train_text_sequences[i]) for i in range(len(train_text_sequences))])/len(train_text_sequences))\n",
    "print(f\"Mean length of articles: {max_length}\")\n",
    "\n",
    "# Use padding to bring sequences into the same size in order to feed them to NLP neural network\n",
    "train_text_padded = pad_sequences(train_text_sequences, maxlen = max_length, padding = padding_type, \n",
    "                                  truncating = trunc_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's see the arguments used for our padding in a little bit more detail:\n",
    "\n",
    "* maxlen: the maximum length of all sequences. We have set this as the mean length of our articles, after they have been cleaned up by removing stopwords, double spaces, punctuation and special characters.\n",
    "* padding: 'pre' or 'post' (optional, defaults to 'pre'). Pad either before or after each sequence. We have set it to `'post'` when we were creating our hyperparameters.\n",
    "* truncating: 'pre' or 'post' (optional, defaults to 'pre'). Remove values from sequences larger than maxlen, either at the beginning or at the end of the sequences. We have set it to `'post'` when we were creating our hyperparameters.\n",
    "\n",
    "As a result, all our articles are now tokenized sequences with length equal to 230 and we achieved that either by deleting all the numbers after the 230th for sequences with length greater than 230, or by adding complementary 0's at the end of the sequence until it reaches a length of 230. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we repreat the exact same process,  this time for the test sequences. Note that we should expect more \"out of vocabulary\" words from articles in the test split because word indexes were derived from the training articles. That is because we fit Tokenizer to training set only, mimicking the fact that unseen words will appear at some point after deploying our model. Thus, model evaluation will be closer to what will happen in an actual production environement.\n",
    "\n",
    "We will also turn the category labels into numpy arrays because that is the form expected from our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning tokens into lists of sequence\n",
    "test_text_sequences = tokenizer.texts_to_sequences(test_text)\n",
    "\n",
    "# Use padding to bring sequences into the same size in order to feed them to NLP neural network\n",
    "test_text_padded = pad_sequences(test_text_sequences, maxlen=max_length, padding=padding_type, \n",
    "                                 truncating=trunc_type)\n",
    "\n",
    "# Turn the categories into numpy arrays\n",
    "train_labels_sequences = np.asarray(train_labels)\n",
    "test_labels_sequences = np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally done with all the necessary preprocessing. Before we move on to creating our document classifiers, it would be fun to see how all of these actions affected our articles through an example. Let's compare one article before and after tokenizing, cleaning and padding. Note that in the first article we have already removed stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: \n",
      "\n",
      "chelsea denied james heroics brave defensive display led keeper david james helped manchester city hold leaders chelsea. quiet opening james denied damien duff jiri jarosik mateja kezman paul bosvelt cleared william gallas header line. robbie fowler scored visitors sent header wide. chelsea possession second half james kept frank lampard free-kick superbly tipped player volley wide. city went game proud record domestic team beat chelsea season. little alarm first 30 minutes chelsea - deprived arjen robben didier drogba injury - struggled pose much threat. indeed visitors looked likelier enliven drab opening played lethargic pace. shaun wright-phillips - watched england boss sven-goran eriksson - showed customary trickery burst right area deliver dangerous ball blocked john terry. chelsea suddenly stepped gear created flurry chances. first duff got round ben thatcher blasted shot james parried kezman turned ball wide. soon afterwards jarosik found space area powerfully head lampard corner goalwards james tipped ball over. chelsea looking like premiership leaders james kept kezman fierce drive bosvelt james combined clear gallas header duff corner. city broke swiftly field last chance frenetic spell resulted fowler celebrating 150th premiership goal. wright-phillips raced left crossed fowler city lone man front left free terry slip contrived head wide seemed breakthrough certain. second half started quietly first although james forced divert cross lively duff away eidur gudjohnsen path. nasty moment petr cech looking ninth straight clean sheet league series ricochets saw fowler chase loose ball area collide accidently czech republic stopper. another quiet spell followed duff interrupted surging run halted illegally edge penalty area bosvelt. lampard stepped blast shot wall james somehow blocked legs. another timely challenge time richard dunne time added prevented gudjohnsen getting shot. still time james produce sensational save tip lampard volley round post. cech paulo ferreira gallas terry bridge jarosik (tiago 56) lampard makelele duff gudjohnsen kezman (cole 63). subs used: johnson smertin cudicini. makelele gudjohnsen. james mills distin dunne thatcher shaun wright-phillips bosvelt barton sibierski (mcmanaman 85) musampa fowler. subs used: macken weaver onuoha jordan. bosvelt. 42 093 h webb (s yorkshire).\n",
      "Length:  2350\n",
      "===================================================================================================================\n",
      "Transformed text: \n",
      "\n",
      "chelsea denied james <UW> <UW> <UW> display led keeper david james helped manchester city hold leaders chelsea <UW> opening james denied damien duff <UW> <UW> <UW> <UW> paul <UW> cleared william gallas header line robbie <UW> scored visitors sent header wide chelsea possession second half james kept frank lampard free kick <UW> tipped player volley wide city went game proud record domestic team beat chelsea season little <UW> first 30 minutes chelsea <UW> <UW> robben <UW> drogba injury struggled <UW> much threat indeed visitors looked <UW> <UW> <UW> opening played <UW> pace shaun wright phillips watched england boss <UW> <UW> <UW> showed <UW> <UW> burst right area deliver dangerous ball blocked john terry chelsea suddenly stepped gear created <UW> chances first duff got round ben thatcher <UW> shot james <UW> <UW> turned ball wide soon afterwards <UW> found space area <UW> head lampard corner <UW> james tipped ball over chelsea looking like premiership leaders james kept <UW> fierce drive <UW> james combined clear gallas header duff corner city broke <UW> field last chance <UW> spell resulted <UW> <UW> <UW> premiership goal wright phillips <UW> left <UW> <UW> city <UW> man front left free terry <UW> <UW> head wide seemed breakthrough certain second half started <UW> first although james forced <UW> cross <UW> duff away <UW> gudjohnsen <UW> <UW> moment <UW> <UW> looking ninth straight clean <UW> league series <UW> saw\n",
      "Length:  230\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_article(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "print(\"Original text:\",'\\n')\n",
    "print(train_text[10])\n",
    "print('Length: ',len(train_text[10]))\n",
    "print(115*'=')\n",
    "print(\"Transformed text:\",'\\n')\n",
    "print(decode_article(train_text_padded[10]))\n",
    "print('Length: ',len(train_text_padded[10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular example, we managed to create a transformed text which is 10 times smaller(!!!) than the original article. But what is really remarkable, is the fact that reading the transformed text we can still understand what it is trying to describe. Hence, we can proudly brag that we achieved our goal: minimize the size of our texts but keeping all of the essential information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings - GloVe\n",
    "\n",
    "Before we proceed with building our neural network we should talk about [word embeddings](https://en.wikipedia.org/wiki/Word_embedding). Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation. This technique works well with deep learning methods. The distributed representation is learned based on the usage of words. This allows words that are used in similar ways to result in having similar representations, naturally capturing their meaning. The whole concept is based on the idea that words that have similar context will have similar meanings. \n",
    "\n",
    "The algorithm which we will be using for learning a word embedding from text data is called [GloVe](https://nlp.stanford.edu/projects/glove/). The Global Vectors for Word Representation, or GloVe, is an extension to the [Word2vec](https://en.wikipedia.org/wiki/Word2vec) (another word embedidng method) for efficiently learning word vectors. GloVe constructs an explicit word-context or word co-occurrence matrix using statistics across the whole text corpus. The result is a learning model that may create better word embeddings.\n",
    "\n",
    "We can either train a new embedding or use a pre-trained embedding on our natural language processing task. Both word2vec and GloVe word embeddings are available for free [download](https://github.com/stanfordnlp/GloVe/blob/master/README.md). We will be using the `glove.6B.100d` version. \n",
    "\n",
    "After we download it, we have to load the entire GloVe word embedding file into memory as a dictionary of word to embedding array. Next, we need to create a matrix of one embedding for each word in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 400000 word vectors in Glove 6B 100d.\n"
     ]
    }
   ],
   "source": [
    "# Loading GloVe to memory as a dict\n",
    "embeddings_index = {};\n",
    "with open('glove.6B.100d.txt',encoding=\"UTF-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split();\n",
    "        word = values[0];\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs;\n",
    "print('Total %s word vectors in Glove 6B 100d.' % len(embeddings_index))\n",
    "\n",
    "# Creating weights matrix\n",
    "embeddings_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embeddings_matrix[i] = embedding_vector       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the models\n",
    "\n",
    "It's finally time to build our document classifiers. As we mentioned at the [read me] file, we will create two neural networks. The first one will be based on a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network) (CNN) and the second one will be based on a [Bidirectional Recurrent Neural Network](https://en.wikipedia.org/wiki/Bidirectional_recurrent_neural_networks) (BRNN). In the end we will compare the results.  \n",
    "\n",
    "For both of our classifiers, as a first layer we will use our pre-trained GloVe embedding. Thus the name \"Embedding Layer\". The size of the vector space is specified as part of the model, such as 50, 100, or 300 dimensions. When we defined our hyperparameters, we set our dimensions to 100. The embedding layer is used on the front end of a neural network and is fit in a supervised way using the [Backpropagation](https://en.wikipedia.org/wiki/Backpropagation) algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN document classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified convolutional neural network\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 230, 100)          2723400   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 226, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 45, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 41, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,886,733\n",
      "Trainable params: 2,886,733\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'num_epochs = 5\\nhistory_cnn = cnn_model.fit(train_text_padded, train_labels_sequences, epochs=num_epochs, \\n                    validation_data=(test_text_padded, test_labels_sequences), verbose=2)\\n\\n# Visualizing the results\\nimport matplotlib.pyplot as plt\\ndef plot_graphs(history_cnn, string):\\n  plt.plot(history_cnn.history[string],linewidth=3.0)\\n  plt.plot(history_cnn.history[\\'val_\\'+string],linewidth=3.0)\\n  plt.xlabel(\"Epochs\",fontsize=14)\\n  plt.ylabel(string,fontsize=14)\\n  plt.legend([\\'Training \\'+string, \\'Validation \\'+string],fontsize=18)\\n  plt.title(string +\\' Curves : CNN\\',fontsize=16)\\n  plt.show()\\n  \\nplot_graphs(history_cnn, \"accuracy\")\\nplot_graphs(history_cnn, \"loss\")'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(len(word_index)+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], \n",
    "                                  trainable=True),\n",
    "        #using a total of 128 filters with size 5 and max pooling of 5 and 35\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(5),\n",
    "        tf.keras.layers.Conv1D(128, 5, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(35),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(len(labels), activation = 'softmax')\n",
    "])\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(\"Simplified convolutional neural network\")\n",
    "cnn_model.summary()\n",
    "\n",
    "# Run this to train the classifier to our data and visualize the results\n",
    "'''num_epochs = 5\n",
    "history_cnn = cnn_model.fit(train_text_padded, train_labels_sequences, epochs=num_epochs, \n",
    "                    validation_data=(test_text_padded, test_labels_sequences), verbose=2)\n",
    "\n",
    "# Visualizing the results\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history_cnn, string):\n",
    "  plt.plot(history_cnn.history[string],linewidth=3.0)\n",
    "  plt.plot(history_cnn.history['val_'+string],linewidth=3.0)\n",
    "  plt.xlabel(\"Epochs\",fontsize=14)\n",
    "  plt.ylabel(string,fontsize=14)\n",
    "  plt.legend(['Training '+string, 'Validation '+string],fontsize=18)\n",
    "  plt.title(string +' Curves : CNN',fontsize=16)\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history_cnn, \"accuracy\")\n",
    "plot_graphs(history_cnn, \"loss\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BRNN document classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 230, 100)          2723400   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 200)               160800    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 2,904,805\n",
      "Trainable params: 2,904,805\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'num_epochs = 5\\nhistory_rnn = rnn_model.fit(train_text_padded, train_labels_sequences, epochs=num_epochs, \\n                    validation_data=(test_text_padded, test_labels_sequences), verbose=2)\\n\\n# -- Visualizing the results -- \\n\\nimport matplotlib.pyplot as plt\\ndef plot_graphs(history, string):\\n  plt.plot(history_rnn.history[string],linewidth=3.0)\\n  plt.plot(history_rnn.history[\\'val_\\'+string],linewidth=3.0)\\n  plt.xlabel(\"Epochs\",fontsize=14)\\n  plt.ylabel(string,fontsize=14)\\n  plt.legend([\\'Training \\'+string, \\'Validation \\'+string],fontsize=18)\\n  plt.title(string +\\' Curves : RNN\\',fontsize=16)\\n  plt.show()\\n  \\nplot_graphs(history, \"accuracy\")\\nplot_graphs(history, \"loss\")'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = tf.keras.Sequential([\n",
    "    # Add an Embedding layer expecting input vocab of a particular size, and output embedding dimension of size 100\n",
    "    tf.keras.layers.Embedding(len(word_index)+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim)),\n",
    "    # used ReLU and tanh function but relu performed better.\n",
    "    tf.keras.layers.Dense(embedding_dim, activation='relu'),\n",
    "    # Add a Dense layer with 5 units and softmax activation.\n",
    "    # When we have multiple outputs, softmax convert outputs layers into a probability distribution.\n",
    "    tf.keras.layers.Dense(5, activation='softmax')\n",
    "])\n",
    "rnn_model.summary()\n",
    "rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Run this to train the classifier to our data and visualize the results\n",
    "'''num_epochs = 5\n",
    "history_rnn = rnn_model.fit(train_text_padded, train_labels_sequences, epochs=num_epochs, \n",
    "                    validation_data=(test_text_padded, test_labels_sequences), verbose=2)\n",
    "\n",
    "# -- Visualizing the results -- \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history_rnn.history[string],linewidth=3.0)\n",
    "  plt.plot(history_rnn.history['val_'+string],linewidth=3.0)\n",
    "  plt.xlabel(\"Epochs\",fontsize=14)\n",
    "  plt.ylabel(string,fontsize=14)\n",
    "  plt.legend(['Training '+string, 'Validation '+string],fontsize=18)\n",
    "  plt.title(string +' Curves : RNN',fontsize=16)\n",
    "  plt.show()\n",
    "  \n",
    "plot_graphs(history, \"accuracy\")\n",
    "plot_graphs(history, \"loss\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td> <img src=\"Screenshots/accuracy_cnn.png\" style=\"width: 300px;\"/> </td>\n",
    "        <td> <img src=\"Screenshots/loss_cnn.png\" style=\"width: 300px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
